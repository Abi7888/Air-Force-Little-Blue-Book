# convert to list
V2 <- list(V2 = V2)
# convert to corpus
V2.corpus <- Corpus(VectorSource(V2))
#######################
# Text Pre-processing #
#######################
## Function(s) for text cleaning
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
## Cleaning the Text:
V2.corpus <- tm_map(V2.corpus, toSpace, "/|@|\\|")
V2.corpus <- tm_map(V2.corpus, toSpace, "[^[:alpha:]]")
V2.corpus <- tm_map(V2.corpus, tolower)
V2.corpus <- tm_map(V2.corpus, removeWords, stopwords("english"))
V2.corpus <- tm_map(V2.corpus, stripWhitespace)
V2.corpus <- tm_map(V2.corpus, stemDocument)
V2.corpus <- str_trim(V2.corpus, side = "both")
## convert back to corpus
V2.corpus <- list(V2.corpus = V2.corpus)
V2.corpus <- Corpus(VectorSource(V2.corpus))
## Developing the Wordcloud:
wordcloud (V2.corpus
, scale=c(4,.05)
, min.freq = 2
, max.words=250
, random.order=FALSE
, random.color = FALSE
, rot.per=0 # % of vertical words
, fixed.asp = TRUE
, use.r.layout=TRUE #Uses C++ for collision detection
#, tryfit=TRUE
#, colors=brewer.pal(8, "Dark2"))
, colors="black")
rm(V2, toSpace)
######################
# Generating n-grams #
######################
options(mc.cores = 1)
V2.corpus <- tm_map(V2.corpus, PlainTextDocument)
# create 1-grams
UnigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
V2.1gram <- TermDocumentMatrix(V2.corpus, control = list(tokenize = UnigramTokenizer))
# turn TermDocumentMatrix into a dataframe
matrix.tdm <- as.matrix(V2.1gram)
df.tdm <- as.data.frame(matrix.tdm, row.names = FALSE)
df.tdm$Token <- rownames(matrix.tdm)
names(df.tdm)[1] <- c("Frequency")
df.tdm <- select(df.tdm, Token, Frequency)
# create full Unigram dataframe
Unigram.df <- df.tdm %>%
group_by(Token) %>%
summarise(Frequency = sum(Frequency)) %>%
arrange(desc(Frequency))
# create 2-grams
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
V2.2gram <- TermDocumentMatrix(V2.corpus, control = list(tokenize = BigramTokenizer))
# turn TermDocumentMatrix into a dataframe
matrix.tdm <- as.matrix(V2.2gram)
df.tdm <- as.data.frame(matrix.tdm, row.names = FALSE)
df.tdm$Token <- rownames(matrix.tdm)
names(df.tdm)[1] <- c("Frequency")
df.tdm <- select(df.tdm, Token, Frequency)
# create full Bigram dataframe
Bigram.df <- df.tdm %>%
group_by(Token) %>%
summarise(Frequency = sum(Frequency)) %>%
arrange(desc(Frequency))
# create 3-grams
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
V2.3gram <- TermDocumentMatrix(V2.corpus, control = list(tokenize = TrigramTokenizer))
# turn TermDocumentMatrix into a dataframe
matrix.tdm <- as.matrix(V2.3gram)
df.tdm <- as.data.frame(matrix.tdm, row.names = FALSE)
df.tdm$Token <- rownames(matrix.tdm)
names(df.tdm)[1] <- c("Frequency")
df.tdm <- select(df.tdm, Token, Frequency)
# create full Trigram dataframe
Trigram.df <- df.tdm %>%
group_by(Token) %>%
summarise(Frequency = sum(Frequency)) %>%
arrange(desc(Frequency))
# create 4-gram
QuadgramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 4, max = 4))
V2.4gram <- TermDocumentMatrix(V2.corpus, control = list(tokenize = QuadgramTokenizer))
# turn TermDocumentMatrix into a dataframe
matrix.tdm <- as.matrix(V2.4gram)
df.tdm <- as.data.frame(matrix.tdm, row.names = FALSE)
df.tdm$Token <- rownames(matrix.tdm)
names(df.tdm)[1] <- c("Frequency")
df.tdm <- select(df.tdm, Token, Frequency)
# create full Unigram dataframe
Quadgram.df <- df.tdm %>%
group_by(Token) %>%
summarise(Frequency = sum(Frequency)) %>%
arrange(desc(Frequency))
rm(df.tdm, matrix.tdm, V2.1gram, V2.2gram, V2.3gram,
V2.4gram, QuadgramTokenizer, BigramTokenizer, TrigramTokenizer, UnigramTokenizer)
##############################
# Visual Analysis of n-grams #
##############################
# Plot unigram
Unigram.Top10 <- Unigram.df %>%
mutate(Rank = seq(from = 1, to = length(Token), by = 1)) %>%
filter(Rank <= 10)
ggplot(Unigram.Top10, aes(Frequency, reorder(Token, -Rank))) +
geom_segment(aes(yend=Token), xend=0, colour="grey50") +
geom_point(size=3) +
theme_bw() +
ylab("Term") +
theme(axis.text.y = element_text(size = 12)) +
ggtitle("Top 15 Most Frequent Words in 1997")
# Plot bigram
Bigram.Top10 <- Bigram.df %>%
mutate(Rank = seq(from = 1, to = length(Token), by = 1)) %>%
filter(Rank <= 15)
ggplot(Bigram.Top10, aes(Frequency, reorder(Token, -Rank))) +
geom_segment(aes(yend=Token), xend=0, colour="grey50") +
geom_point(size=3) +
theme_bw() +
ylab("Term") +
theme(axis.text.y = element_text(size = 12)) +
ggtitle("Top 10 Bi-grams in 1997")
# Plot trigram
Trigram.Top10 <- Trigram.df %>%
mutate(Rank = seq(from = 1, to = length(Token), by = 1)) %>%
filter(Rank <= 5)
ggplot(Trigram.Top10, aes(Frequency, reorder(Token, -Rank))) +
geom_segment(aes(yend=Token), xend=0, colour="grey50") +
geom_point(size=3) +
theme_bw() +
ylab("Term") +
theme(axis.text.y = element_text(size = 12)) +
ggtitle("Top 5 Most Frequent Tri-grams in 1997")
# Plot quadgram
Quadgram.Top10 <- Quadgram.df %>%
mutate(Rank = seq(from = 1, to = length(Token), by = 1)) %>%
filter(Rank <= 5)
ggplot(Quadgram.Top10, aes(Frequency, reorder(Token, -Rank))) +
geom_segment(aes(yend=Token), xend=0, colour="grey50") +
geom_point(size=3) +
theme_bw() +
ylab("Term") +
theme(axis.text.y = element_text(size = 12)) +
ggtitle("Top 5 Most Frequent 4-grams in 1997")
library(tm)
library(wordcloud)
library(SnowballC)
library(ggplot2)
# library(Rgraphviz) ##Need to install
library(stringr)
library(RWeka)
library(dplyr)
library(tidyr)
library(gridExtra)
library(ggplot2) # plot word frequencies
library(scales) # format axis scales for plots
# pull in text data
V2 <- read.csv("~/Desktop/R/Air Force Little Blue Book/1997r.txt", header=FALSE, comment.char="#", stringsAsFactors = FALSE)
# convert to list
V2 <- list(V2 = V2)
# convert to corpus
V2.corpus <- Corpus(VectorSource(V2))
#######################
# Text Pre-processing #
#######################
## Function(s) for text cleaning
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
## Cleaning the Text:
V2.corpus <- tm_map(V2.corpus, toSpace, "/|@|\\|")
V2.corpus <- tm_map(V2.corpus, toSpace, "[^[:alpha:]]")
V2.corpus <- tm_map(V2.corpus, tolower)
V2.corpus <- tm_map(V2.corpus, removeWords, stopwords("english"))
V2.corpus <- tm_map(V2.corpus, stripWhitespace)
V2.corpus <- tm_map(V2.corpus, stemDocument)
V2.corpus <- str_trim(V2.corpus, side = "both")
## convert back to corpus
V2.corpus <- list(V2.corpus = V2.corpus)
V2.corpus <- Corpus(VectorSource(V2.corpus))
## Developing the Wordcloud:
wordcloud (V2.corpus
, scale=c(4,.05)
, min.freq = 2
, max.words=250
, random.order=FALSE
, random.color = FALSE
, rot.per=0 # % of vertical words
, fixed.asp = TRUE
, use.r.layout=TRUE #Uses C++ for collision detection
#, tryfit=TRUE
#, colors=brewer.pal(8, "Dark2"))
, colors="black")
rm(V2, toSpace)
######################
# Generating n-grams #
######################
options(mc.cores = 1)
V2.corpus <- tm_map(V2.corpus, PlainTextDocument)
# create 1-grams
UnigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
V2.1gram <- TermDocumentMatrix(V2.corpus, control = list(tokenize = UnigramTokenizer))
# turn TermDocumentMatrix into a dataframe
matrix.tdm <- as.matrix(V2.1gram)
df.tdm <- as.data.frame(matrix.tdm, row.names = FALSE)
df.tdm$Token <- rownames(matrix.tdm)
names(df.tdm)[1] <- c("Frequency")
df.tdm <- select(df.tdm, Token, Frequency)
# create full Unigram dataframe
Unigram.df <- df.tdm %>%
group_by(Token) %>%
summarise(Frequency = sum(Frequency)) %>%
arrange(desc(Frequency))
# create 2-grams
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
V2.2gram <- TermDocumentMatrix(V2.corpus, control = list(tokenize = BigramTokenizer))
# turn TermDocumentMatrix into a dataframe
matrix.tdm <- as.matrix(V2.2gram)
df.tdm <- as.data.frame(matrix.tdm, row.names = FALSE)
df.tdm$Token <- rownames(matrix.tdm)
names(df.tdm)[1] <- c("Frequency")
df.tdm <- select(df.tdm, Token, Frequency)
# create full Bigram dataframe
Bigram.df <- df.tdm %>%
group_by(Token) %>%
summarise(Frequency = sum(Frequency)) %>%
arrange(desc(Frequency))
# create 3-grams
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
V2.3gram <- TermDocumentMatrix(V2.corpus, control = list(tokenize = TrigramTokenizer))
# turn TermDocumentMatrix into a dataframe
matrix.tdm <- as.matrix(V2.3gram)
df.tdm <- as.data.frame(matrix.tdm, row.names = FALSE)
df.tdm$Token <- rownames(matrix.tdm)
names(df.tdm)[1] <- c("Frequency")
df.tdm <- select(df.tdm, Token, Frequency)
# create full Trigram dataframe
Trigram.df <- df.tdm %>%
group_by(Token) %>%
summarise(Frequency = sum(Frequency)) %>%
arrange(desc(Frequency))
# create 4-gram
QuadgramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 4, max = 4))
V2.4gram <- TermDocumentMatrix(V2.corpus, control = list(tokenize = QuadgramTokenizer))
# turn TermDocumentMatrix into a dataframe
matrix.tdm <- as.matrix(V2.4gram)
df.tdm <- as.data.frame(matrix.tdm, row.names = FALSE)
df.tdm$Token <- rownames(matrix.tdm)
names(df.tdm)[1] <- c("Frequency")
df.tdm <- select(df.tdm, Token, Frequency)
# create full Unigram dataframe
Quadgram.df <- df.tdm %>%
group_by(Token) %>%
summarise(Frequency = sum(Frequency)) %>%
arrange(desc(Frequency))
rm(df.tdm, matrix.tdm, V2.1gram, V2.2gram, V2.3gram,
V2.4gram, QuadgramTokenizer, BigramTokenizer, TrigramTokenizer, UnigramTokenizer)
##############################
# Visual Analysis of n-grams #
##############################
# Plot unigram
Unigram.Top10 <- Unigram.df %>%
mutate(Rank = seq(from = 1, to = length(Token), by = 1)) %>%
filter(Rank <= 10)
ggplot(Unigram.Top10, aes(Frequency, reorder(Token, -Rank))) +
geom_segment(aes(yend=Token), xend=0, colour="grey50") +
geom_point(size=3) +
theme_bw() +
ylab("Term") +
theme(axis.text.y = element_text(size = 12)) +
ggtitle("Top 15 Most Frequent Words in 1997")
# Plot bigram
Bigram.Top10 <- Bigram.df %>%
mutate(Rank = seq(from = 1, to = length(Token), by = 1)) %>%
filter(Rank <= 5)
ggplot(Bigram.Top10, aes(Frequency, reorder(Token, -Rank))) +
geom_segment(aes(yend=Token), xend=0, colour="grey50") +
geom_point(size=3) +
theme_bw() +
ylab("Term") +
theme(axis.text.y = element_text(size = 12)) +
ggtitle("Top 5 Bi-grams in 1997")
# Plot trigram
Trigram.Top10 <- Trigram.df %>%
mutate(Rank = seq(from = 1, to = length(Token), by = 1)) %>%
filter(Rank <= 5)
ggplot(Trigram.Top10, aes(Frequency, reorder(Token, -Rank))) +
geom_segment(aes(yend=Token), xend=0, colour="grey50") +
geom_point(size=3) +
theme_bw() +
ylab("Term") +
theme(axis.text.y = element_text(size = 12)) +
ggtitle("Top 5 Most Frequent Tri-grams in 1997")
# Plot quadgram
Quadgram.Top10 <- Quadgram.df %>%
mutate(Rank = seq(from = 1, to = length(Token), by = 1)) %>%
filter(Rank <= 5)
ggplot(Quadgram.Top10, aes(Frequency, reorder(Token, -Rank))) +
geom_segment(aes(yend=Token), xend=0, colour="grey50") +
geom_point(size=3) +
theme_bw() +
ylab("Term") +
theme(axis.text.y = element_text(size = 12)) +
ggtitle("Top 5 Most Frequent 4-grams in 1997")
setwd("~/Desktop/R/Air Force Little Blue Book")
install.packages("~/Desktop/R/Air Force Little Blue Book/Rstem_0.4-1.tar", repos = NULL, type = "source")
install.packages("~/Desktop/R/Air Force Little Blue Book/sentiment_0.2.tar.gz", repos = NULL, type = "source")
install.packages("plyr")
library(plyr)
library(Rstem)
library(sentiment)
library(tm)
library(stringr)
#Document Input and basic analysis
V1 <- scan("~/Desktop/R/Air Force Little Blue Book/2015r.txt", what="character", sep="\n")
V1.lower <- tolower(V1)
length(V1.lower)
V1.words.l <- strsplit(V1.lower, "\\W")
V1.words.l
V1.words.v <- unlist(V1.words.l)
not.blanks.v <- which(V1.words.v!="")
not.blanks.v
V1.words.v <- V1.words.v[not.blanks.v]
V1.words.v
V1.ws <- paste(V1.words.v,sept=" ",collapse=" ")
V1.ws
#R Sentiment Analysis for Polarity of the Text:
Polarity <- classify_polarity(V1.ws, algorithm = "bayes")
Polarity
V1 <- scan("1983r.txt", what="character", sep="\n")
V1
V1.lower <- tolower(V1)
length(V1.lower)
V1.words.l <- strsplit(V1.lower, "\\W")
V1.words.l
V1.words.v <- unlist(V1.words.l)
V1.words.v
not.blanks.v <- which(V1.words.v!="")
not.blanks.v
V1.words.v <- V1.words.v[not.blanks.v]
V1.words.v
length(V1.words.v[which(V1.words.v=="you")])
length(V1.words.v[which(V1.words.v=="our")])
length(V1.words.v)
library(magrittr)
"1983r.txt"
scan("1983r.txt", what="character", sep="\n") %>%
tolower(.)
scan("1983r.txt", what="character", sep="\n") %>%
tolower(.) %>%
strsplit("\\W") %>%
unlist(.)
scan("1983r.txt", what="character", sep="\n") %>%
tolower(.) %>%
strsplit("\\W") %>%
unlist(.) %>%
.[. != ""]
V1.words.v
scan("1983r.txt", what="character", sep="\n") %>%
tolower(.) %>%
strsplit("\\W") %>%
unlist(.) %>%
.[. != ""]
length(V1.words.v[which(V1.words.v=="you")])
"1983r.txt" %>%
scan(what="character", sep="\n") %>%
tolower(.) %>%
strsplit("\\W") %>%
unlist(.) %>%
.[. != ""]
"1983r.txt" %>%
scan(., what="character", sep="\n") %>%
tolower(.) %>%
strsplit("\\W") %>%
unlist(.) %>%
.[. != ""]
library(magrittr)
"1983r.txt" %>%
scan(., what="character", sep="\n") %>%
tolower(.) %>%
strsplit("\\W") %>%
unlist(.) %>%
.[. != ""]
"1983r.txt" %>%
scan(what="character", sep="\n") %>%
tolower(.) %>%
strsplit("\\W") %>%
unlist(.) %>%
.[. != ""]
V1 <- scan("1983r.txt", what="character", sep="\n")
V1.lower <- tolower(V1)
length(V1.lower)
V1.words.l <- strsplit(V1.lower, "\\W")
V1.words.l
V1.words.v <- unlist(V1.words.l)
not.blanks.v <- which(V1.words.v!="")
not.blanks.v
V1.words.v <- V1.words.v[not.blanks.v]
V1.words.v
length(V1.words.v[which(V1.words.v=="you")])
words_v %>%
.[. == "you"] %>%
length(.)
words_v <- "1983r.txt" %>%
scan(what="character", sep="\n") %>%
tolower(.) %>%
strsplit("\\W") %>%
unlist(.) %>%
.[. != ""]
words_v %>%
.[. == "you"] %>%
length(.)
words_v %>%
.[. %in% c("you", "our")]
length(V1.words.v[which(V1.words.v=="our")])
?table
words_v %>%
.[. %in% c("you", "our")] %>%
table(.)
length(V1.words.v[which(V1.words.v=="our")])
words_v %>%
table(.)
?assign
words_v %>%
table(.) %>%
assign(total, length(.))
words_v %>%
table(.) %>%
assign(x="total",value=sum(.),pos=1)
words_v %>%
table(.)
a <- 1:4
assign("a[1]", 2)
a
assign(x="total",value=sum(length(words_v)),pos=1)
words_v %>%
table(.) %>%
assign(x="total.words.v",length(words_v),pos=1) %>%
.[. %in% c("you", "our")]
words_v %>%
table(.)
words_v %>%
table(.) %>% sum(.)
words_v %>%
table(.) %>%
assign(x="total.words.v",sum(.),pos=1)
words_v %>%
table(.) %>%
.[. %in% c("you", "our")]
words_v %>%
table(.)
words_v %>%
.[. %in% c("you", "our")] %>%
table(.)
words_v %>%
.[. %in% c("you", "our")] %>%
table(.) %>%
`rownames<-`(., "count")`
words_v %>%
.[. %in% c("you", "our")] %>%
table(.) %>%
row.names <- (., "count")
?row.names
words_v %>%
.[. %in% c("you", "our")] %>%
table(.) %>%
row.names(.) <- "count"
words_v %>%
.[. %in% c("you", "our")] %>%
table(.) %>%
row.names(.)
a <- words_v %>%
.[. %in% c("you", "our")] %>%
table(.)
a
a[our]
a["our"]
key_word <- list()
key_word$count <- words_v %>%
.[. %in% c("you", "our")] %>%
table(.)
key_word
str(key_word)
key_word$count/2
key_word$ratio <- key_word$count/length(words_v)
key_word$ratio
key_word
?order
words_v %>%
table(.) %>%
order(.)
words_v %>%
table(.) %>%
sort(.)
words_v %>%
table(.) %>%
sort(., decreasing = FALSE)
words_v %>%
table(.) %>%
sort(., decreasing = TRUE)
words_v %>%
table(.) %>%
sort(., decreasing = TRUE) %>%
.[1:10]
words_v %>%
table(.) %>%
sort(., decreasing = TRUE) %>%
.[1:20]
# Top ten words
key_word$top10 <- words_v %>%
table(.) %>%
sort(., decreasing = TRUE) %>%
.[1:10]
key_word
